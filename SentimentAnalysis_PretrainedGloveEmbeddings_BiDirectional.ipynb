{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Dataset\n",
    "Source: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /anaconda3/lib/python3.6/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from spacy) (39.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.4.16)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /anaconda3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.31.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /anaconda3/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /anaconda3/lib/python3.6/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.18.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (39.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2018.4.16)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /anaconda3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.31.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID Sentiment SentimentSource  \\\n",
       "0       1       neg    Sentiment140   \n",
       "1       2       neg    Sentiment140   \n",
       "2       3       pos    Sentiment140   \n",
       "3       4       neg    Sentiment140   \n",
       "4       5       neg    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3  .. Omgaga. Im sooo  im gunna CRy. I've been at...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('datasets/tweets/tweets.csv', error_bad_lines = False)\n",
    "\n",
    "tweets = tweets.head(50000)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe consists of 4 columns and we want to use only ‘Sentiment’ and ‘SentimentText’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                      SentimentText\n",
       "0       neg                       is so sad for my APL frie...\n",
       "1       neg                     I missed the New Moon trail...\n",
       "2       pos                            omg its already 7:30 :O\n",
       "3       neg  .. Omgaga. Im sooo  im gunna CRy. I've been at...\n",
       "4       neg           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets  = tweets.drop(columns = ['ItemID', 'SentimentSource'], axis = 1)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    26921\n",
       "neg    23079\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5,0,'Labels')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = sns.barplot(x=tweets.Sentiment.unique(), y=tweets.Sentiment.value_counts())\n",
    "\n",
    "ax.set(xlabel='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(tweets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Sentiment                                      SentimentText\n",
       " 0           pos  @amyrenea omg so am I lol I fell asleep when i...\n",
       " 1           neg               @Adrienne_Bailon I want a shout out \n",
       " 2           neg  @Anonymousboy03 Plans for school stuff &amp; a...\n",
       " 3           neg  ... has hit a writer's block .. am loosing my ...\n",
       " 4           neg  ... trying to find people I know! I`m bored, i...\n",
       " 5           pos  &quot;Rise from your grave!&quot;ï¿½finalmente...\n",
       " 6           pos                    @1RedDiamond it is where I live\n",
       " 7           neg       @anthonystonem But I'm almost as fat as her \n",
       " 8           pos  @Addicted2Golf I am getting addicted.  Almost ...\n",
       " 9           pos  @Abongachong Ahem, ignore the last tweet. Thou...\n",
       " 10          pos                          *cough* LOL. Okay.  @LOXX\n",
       " 11          pos                        @_huny enter the zoom lens \n",
       " 12          pos  ....ok, sOooooooooo I DID that!! i've got u in...\n",
       " 13          pos  @28parkave well done you! I need to do my meas...\n",
       " 14          neg  @alunvaughan I think a lot aren't really in fo...\n",
       " 15          pos  ..Listening to &quot;Glamourous&quot; by Fergi...\n",
       " 16          pos           @AlphaNorth 9:30/10. I have a ride today\n",
       " 17          pos  @AlexAlas I'm in the mood for a Flinstones vit...\n",
       " 18          pos  @adamrocks my uncle picked up some there yeste...\n",
       " 19          neg  #inaperfectworld @dwighthoward would be wearin...\n",
       " 20          neg             stupid texts being late for no reason.\n",
       " 21          neg  @AlexanderSpit I don't have the energy to put ...\n",
       " 22          neg                         @ajbutie not yet  his loss\n",
       " 23          neg  &quot;StumbleUpon has temporarily run out of s...\n",
       " 24          pos  ??? MAY SALE!!! 30% OFF EVERYTHING @ RED METAL...\n",
       " 25          pos        - you're welcome  http://aweber.com/b/22u8Z\n",
       " 26          pos  #Goodsex When your partner follows you on mysp...\n",
       " 27          pos  &quot;i am a vampire, i am a vampire...but i h...\n",
       " 28          neg  @@vooveth carpooltunnel i think!  how lame &am...\n",
       " 29          pos  ...It's because you have to know!!! Grand Open...\n",
       " ...         ...                                                ...\n",
       " 39970       pos  @copicmarker&quot; facebook.com/copic.marker i...\n",
       " 39971       pos   I just don't know how to begin this twitter t...\n",
       " 39972       pos  .@fourchickens I just swan around in them with...\n",
       " 39973       pos  @_alii yaaaay  haha i think it could be a good...\n",
       " 39974       pos  @615Redbone ham, turkey, cheese, amd mayo plea...\n",
       " 39975       neg  @andwhenyousing unless adam's 6ft 3 ness beat ...\n",
       " 39976       pos                                @agiftedmind YAAAY \n",
       " 39977       pos          #favoritelyrics anything by LUPE FIASCO. \n",
       " 39978       pos  @aj_michalka No problems Twitter rocks  im a t...\n",
       " 39979       neg                 @ i wanna go home, so miss my mom \n",
       " 39980       neg     ...I've got the biggest crush on Kevin James. \n",
       " 39981       pos  @Alybean I suffer from a very bad case of fing...\n",
       " 39982       pos   @alliewayfilms No I won't  I want to follow you \n",
       " 39983       pos         #follow @spreadingjoy the name say it all \n",
       " 39984       neg   i am regretting getting that film developed.....\n",
       " 39985       pos   @LisaBarone If, thru the miracle of modern sc...\n",
       " 39986       neg   I keep getting errors on iPhone and it saves ...\n",
       " 39987       pos  @andrewfaith sorry, old Habits are hard to break \n",
       " 39988       neg  .I feel so mcuh preasure and no answers in my ...\n",
       " 39989       pos  @ankita_gaba Get well soon and no working on s...\n",
       " 39990       neg  @AQuietMadness  just block everyone out. Can y...\n",
       " 39991       neg  @3EG nothing just inside b/c its raining  yup ...\n",
       " 39992       pos  @amabaie Oh no 'm already there I can't be you...\n",
       " 39993       neg  ... Tried really hard to stay and socialize bu...\n",
       " 39994       neg  #dontyouhate when you try to be sumbody everyt...\n",
       " 39995       pos   #robotpickuplines are so funny. check them out. \n",
       " 39996       pos  @annyo84 awh thankss.  yeah, i understand what...\n",
       " 39997       pos  @AmbiguityX ohh you're in twin cities?  i luv ...\n",
       " 39998       neg   Dinara lost again in Roland Garros. Why the S...\n",
       " 39999       pos  *yawn* fucking time zones shit. I'm really sic...\n",
       " \n",
       " [40000 rows x 2 columns],\n",
       "      Sentiment                                      SentimentText\n",
       " 0          pos  @aimeesays aww i hope it does fly by because J...\n",
       " 1          neg  #dontyouhate when you JUST painted yur nails a...\n",
       " 2          neg  - @EvertB which one? http://bit.ly/10o8LW, htt...\n",
       " 3          pos  *shriek* Bee almost flew here from window. I'm...\n",
       " 4          pos  @Alyssa_Milano granted if we lose it is to a w...\n",
       " 5          neg  @AnG_CaKe i dont have a choice man. there'll b...\n",
       " 6          pos           #jonaswebcast Cant wait for another one \n",
       " 7          neg   @AshleyNikole4 awww. Look at u all sad and shit!\n",
       " 8          neg                              why? GOODNIGHT WORLD.\n",
       " 9          neg                    @AliC66 NOT GOING  no friends x\n",
       " 10         pos  @AngelicaPreston  He is my favorite person of ...\n",
       " 11         pos  @amoxyspasm you talking bout us or the actual ...\n",
       " 12         neg   skool holidays are over and skool sux but we ...\n",
       " 13         neg                            but why not? @kathrynYO\n",
       " 14         pos  .@MyInnerChild oh shit i'm sorry Jules!!! i'd ...\n",
       " 15         neg  @ak618 maybe for the best Torres didn't play--...\n",
       " 16         pos                                       ..back home \n",
       " 17         neg  @ak618 no  it's the 1 game in the homestand I'...\n",
       " 18         neg  *sniffles* I wish I could go to Camp Broadway ...\n",
       " 19         pos        *editing some NEW photos. Late aft lovies. \n",
       " 20         neg   sad because MTV music awards isn't being aire...\n",
       " 21         neg  . . . yeah fuck that shit. Also kind of bored ...\n",
       " 22         pos   church with matt,was...good. I think I'm goin...\n",
       " 23         pos  (Tip: Good music- and hot tub, if possible- wi...\n",
       " 24         neg                    ... procrastinating in the net \n",
       " 25         pos  @AdrianneCurry Did you know that Lionel Ritchi...\n",
       " 26         pos        @andy_lamb yeah that one might have a kick \n",
       " 27         pos                             @adarh I'll next time \n",
       " 28         neg  @alineab Eeew-off to airport &amp; whoever djt...\n",
       " 29         neg  @aianna21 And booo, I want twin tiiiiime. It's...\n",
       " ...        ...                                                ...\n",
       " 9970       neg  @5by5forever Dang it im jealous! I want to see...\n",
       " 9971       neg  @ashleybella we'll blast the CD on the way the...\n",
       " 9972       neg  @ainojonas yes I know I can't.  but I just can...\n",
       " 9973       neg      @aliaargh sorry aly, I dont go to your party \n",
       " 9974       pos                               @ajmclean_team Yep. \n",
       " 9975       neg  #Blink182 tickets went on sale in Phoenix. 55$...\n",
       " 9976       pos  @alexzealand  Thank ya kindly!  I'm thinking o...\n",
       " 9977       neg  @acidnation stop angryfacing me  you shoulda j...\n",
       " 9978       neg    @ work. on day #6.  whts goin on with everyone?\n",
       " 9979       pos                 @AndyGroenink God bless you, man! \n",
       " 9980       neg       @apollo_b4 I was up... But now I'm in class.\n",
       " 9981       neg  @andreatrasatti at this point, I guess I would...\n",
       " 9982       neg  @Alyssa_Milano did you see the new Star Trek m...\n",
       " 9983       pos  @amybaby63 Yep. 16th June is my last exam (med...\n",
       " 9984       pos                  #or09 MattZ admits to Teh Clumsy.\n",
       " 9985       pos                  @akiville how much u ask for it? \n",
       " 9986       pos  @acttdanceesingg hey carly! XD my names carly ...\n",
       " 9987       pos  @alliegirl97 yep lol heard that once from a ga...\n",
       " 9988       pos  @archreena Nako, matulog ka na. I'll do the pr...\n",
       " 9989       pos   #followFriday @Angel42579 &lt;-- she likes Vegas\n",
       " 9990       pos  @Adam9309 good morning my wee darling!...*waving*\n",
       " 9991       pos  @angiedarintip i'm only gonna watch this cuz i...\n",
       " 9992       neg  .@paloguitars But I luffs you! Only told you y...\n",
       " 9993       pos          @alli_cat141 i know!  shoes are good! *.*\n",
       " 9994       neg               @aeayling   jk ill take any of them!\n",
       " 9995       neg  @aisforamylynn you're a badass for having a ba...\n",
       " 9996       pos  @acts_rox  I'm not particular about it being f...\n",
       " 9997       pos                     @@j311stp and the same to you!\n",
       " 9998       pos  .@nanere Sheila I heart you!! That &quot;Holly...\n",
       " 9999       neg   not the same without a goodnight....hm. Wish ...\n",
       " \n",
       " [10000 rows x 2 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39087</th>\n",
       "      <td>pos</td>\n",
       "      <td>@amyrenea omg so am I lol I fell asleep when i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Adrienne_Bailon I want a shout out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Anonymousboy03 Plans for school stuff &amp;amp; a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>neg</td>\n",
       "      <td>... has hit a writer's block .. am loosing my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>neg</td>\n",
       "      <td>... trying to find people I know! I`m bored, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                      SentimentText\n",
       "39087       pos  @amyrenea omg so am I lol I fell asleep when i...\n",
       "30893       neg               @Adrienne_Bailon I want a shout out \n",
       "45278       neg  @Anonymousboy03 Plans for school stuff &amp; a...\n",
       "16398       neg  ... has hit a writer's block .. am loosing my ...\n",
       "13653       neg  ... trying to find people I know! I`m bored, i..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 2), (10000, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('datasets/tweets/train_tweets.csv', index=False)\n",
    "test.to_csv('datasets/tweets/test_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtest_tweets.csv\u001b[m\u001b[m  \u001b[31mtrain_tweets.csv\u001b[m\u001b[m \u001b[31mtweets.csv\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining a funtion to clean the tweets by removing non alphanumeric character and links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_clean(text):\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) \n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) \n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The tweet column (‘SentimentText’) needs processing and tokenization, so that it can be converted into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = tokenizer)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('Sentiment', LABEL), ('SentimentText', TEXT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create torchtext dataset,TabularDataset which is specially designed to read csv and tsv files and process them. It is a wrapper around pytorch Dataset with additional features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = torchtext.data.TabularDataset.splits(path = 'datasets/tweets/', \n",
    "                                                train = 'train_tweets.csv',\n",
    "                                                test = 'test_tweets.csv',    \n",
    "                                                format = 'csv',\n",
    "                                                skip_header = True,\n",
    "                                                fields = datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(trn)}')\n",
    "print(f'Number of testing examples: {len(tst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'pos',\n",
       " 'SentimentText': ['amyrenea',\n",
       "  'omg',\n",
       "  'so',\n",
       "  'am',\n",
       "  'i',\n",
       "  'lol',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'when',\n",
       "  'it',\n",
       "  'was',\n",
       "  'on',\n",
       "  'last',\n",
       "  'night',\n",
       "  'so',\n",
       "  'now',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'finish',\n",
       "  'it']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trn.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'pos',\n",
       " 'SentimentText': ['aimeesays',\n",
       "  'aww',\n",
       "  'i',\n",
       "  'hope',\n",
       "  'it',\n",
       "  'does',\n",
       "  'fly',\n",
       "  'by',\n",
       "  'because',\n",
       "  'jt',\n",
       "  'episodes',\n",
       "  'are',\n",
       "  'usually',\n",
       "  'really',\n",
       "  'good',\n",
       "  'and',\n",
       "  'it',\n",
       "  's',\n",
       "  'early',\n",
       "  'but',\n",
       "  'so',\n",
       "  'far',\n",
       "  'this',\n",
       "  'ep',\n",
       "  'hassn',\n",
       "  't',\n",
       "  'disappointed']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(tst.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained word vectors and build vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [07:48, 1.84MB/s]                               \n",
      "100%|█████████▉| 399310/400000 [00:13<00:00, 30728.19it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(trn, max_size=25000,\n",
    "                 vectors=\"glove.6B.100d\",\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 25644), ('the', 12219), ('to', 12111), ('you', 10723), ('a', 9197), ('it', 8440), ('and', 6889), ('my', 6208), ('quot', 5582), ('s', 5564), ('that', 5306), ('is', 5203), ('for', 4971), ('in', 4852), ('t', 4844), ('m', 4683), ('me', 4588), ('of', 4331), ('on', 3918), ('have', 3752), ('so', 3612), ('but', 3506), ('be', 2932), ('not', 2887), ('was', 2775), ('just', 2724), ('can', 2523), ('do', 2418), ('are', 2351), ('your', 2320), ('with', 2269), ('good', 2203), ('like', 2173), ('at', 2131), ('no', 2119), ('this', 2093), ('all', 2069), ('up', 2066), ('now', 2063), ('get', 2044), ('we', 1988), ('u', 1890), ('love', 1885), ('lol', 1864), ('too', 1826), ('what', 1760), ('out', 1742), ('know', 1664), ('nt', 1608), ('amp', 1539)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'i', 'the', 'to', 'you', 'a', 'it', 'and', 'my']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data in batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "                                (trn, tst),\n",
    "                                batch_size = 64,\n",
    "                                sort_key=lambda x: len(x.SentimentText),\n",
    "                                sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                 output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                           bidirectional = bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "       \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim, \n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            output_dim, \n",
    "            n_layers, \n",
    "            bidirectional, \n",
    "            dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25002, 100)\n",
       "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the embeddings from the field's vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the initial weights of the embedding layer with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4800,  2.1939,  0.0493,  ..., -0.6837, -1.1314,  0.5929],\n",
       "        [-0.8291, -1.3337,  0.6655,  ...,  0.7658,  0.8194,  1.1231],\n",
       "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
       "        ...,\n",
       "        [-0.2661, -0.0154, -1.4533,  ...,  0.4913,  0.5435,  0.7071],\n",
       "        [ 0.4209,  0.3358, -0.9088,  ...,  0.7452, -1.1715,  0.6153],\n",
       "        [-0.2440, -1.7382, -0.5827,  ...,  0.0857, -1.8108,  0.6854]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
      "        ...,\n",
      "        [-0.2661, -0.0154, -1.4533,  ...,  0.4913,  0.5435,  0.7071],\n",
      "        [ 0.4209,  0.3358, -0.9088,  ...,  0.7452, -1.1715,  0.6153],\n",
      "        [-0.2440, -1.7382, -0.5827,  ...,  0.0857, -1.8108,  0.6854]])\n"
     ]
    }
   ],
   "source": [
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "We use Adam optimizer and loss function is BCEWithLogitLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define a function for training our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "        \n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.Sentiment).float() \n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 399310/400000 [00:30<00:00, 30728.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.627 | Train Acc: 64.33% |\n",
      "| Epoch: 02 | Train Loss: 0.540 | Train Acc: 73.57% |\n",
      "| Epoch: 03 | Train Loss: 0.497 | Train Acc: 76.34% |\n",
      "| Epoch: 04 | Train Loss: 0.467 | Train Acc: 78.23% |\n",
      "| Epoch: 05 | Train Loss: 0.446 | Train Acc: 79.63% |\n",
      "| Epoch: 06 | Train Loss: 0.427 | Train Acc: 80.58% |\n",
      "| Epoch: 07 | Train Loss: 0.408 | Train Acc: 81.56% |\n",
      "| Epoch: 08 | Train Loss: 0.394 | Train Acc: 82.40% |\n",
      "| Epoch: 09 | Train Loss: 0.379 | Train Acc: 83.52% |\n",
      "| Epoch: 10 | Train Loss: 0.362 | Train Acc: 84.13% |\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.494 | Test Acc: 77.18%\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_iterator:\n",
    "\n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.Sentiment).float() \n",
    "        \n",
    "        acc = correct.sum()/len(correct)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "test_loss = epoch_loss / len(test_iterator)\n",
    "test_acc = epoch_acc / len(test_iterator)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Input\n",
    "We can now use our model to predict the sentiment of any sentence we give it.As it has been trained on tweets, the sentences provided should in a positive or a negative context.\n",
    "\n",
    "We are expecting tweets with a negative sentiment to return a value close to 1 and positive tweets to return a value close to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I hate that show' \n",
    "\n",
    "#Run again for \n",
    "#\"That movie was really nice\"\n",
    "\n",
    "#\"I hate that show but recently it has been quite good\"\n",
    " \n",
    "#\"That movie was decent but kind of fizzled out towards the end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_predict(sen):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sen)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    print(sen)\n",
    "    print(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate that show\n",
      "0.8901607394218445\n"
     ]
    }
   ],
   "source": [
    "test_predict('I hate that show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That movie was really nice\n",
      "0.19563615322113037\n"
     ]
    }
   ],
   "source": [
    "test_predict('That movie was really nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate that show but recently it has been quite good\n",
      "0.8760472536087036\n"
     ]
    }
   ],
   "source": [
    "test_predict('I hate that show but recently it has been quite good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
